- [S02.05  worksheet on MongoDB](#s0205--worksheet-on-mongodb)
  - [The context](#the-context)
  - [Plan](#plan)
    - [dataset samples](#dataset-samples)
    - [Expectations and work](#expectations-and-work)
  - [Step 1: trees and flat schema](#step-1-trees-and-flat-schema)
    - [Create a new database](#create-a-new-database)
  - [Let's load the data](#lets-load-the-data)
    - [Using mongoimport](#using-mongoimport)
    - [Queries](#queries)
  - [different schema -](#different-schema--)
  - [](#)
  - [practice](#practice)
  - [conversations](#conversations)
    - [convert gardens geolocation](#convert-gardens-geolocation)
    - [transform point to GeoJSON](#transform-point-to-geojson)
    - [create collection, schema validator](#create-collection-schema-validator)

# S02.05  worksheet on MongoDB

## The context

Trees are a huge asset in the path to resilience with regard to climate change.

The city of Paris has a strong policy to use trees to lower the temperatures during heat waves.

"La Ville s’est fixé un objectif de débitumiser 100 hectares et de planter 170 000 arbres entre 2020 et 2026. "
https://www.paris.fr/pages/paris-s-adapte-au-changement-climatique-18541#plus-de-vert-moins-de-bitume-et-plus-d-arbres

see also "le plan arbres" https://cdn.paris.fr/paris/2021/12/13/daf6cce214190a66c7919b34989cf1ed.pdf

You want to launch a new app where people can find shade when the temperature rises.
Given a user location and the app finds the closest garden with the largest amount of trees.

You have an important meeting with the Paris team in charge of trees where you want to demonstrate a POC (proof of concept) of your new app.

Fortunately, there is a couple of datasets you can leverage available on the Paris open data platform

- The [Paris Tree dataset](https://opendata.paris.fr/explore/dataset/les-arbres/information/) that includes over 210k tress in and around Paris.
- The "[Espaces verts](https://opendata.paris.fr/explore/dataset/espaces_verts/information/)" dataset (green spaces, aka gardens) with over 2300 areas.

The first phase of the POC, plan is to use these 2 datasets to figure out how many trees each Espace vert, green area, contains.

These are real world datasets and as such have their share of anomalies.

In today's workshop you will

- load both datasets into MongoDB collections and compare schema designs
- clean up the data by creating validation rules
- learn to use [GeoJSON](https://en.wikipedia.org/wiki/GeoJSON)
- optimize some queries by creating indexes
- find the best schema pattern to reconcile both dataset in one.

## Plan

The plan of this workshop

1. flat schema scenario for the trees dataset
   1. create database, import data directly from json file
   2. insert data, time it
   3. some queries
2. nested schema scenario for the trees dataset
   1. create schema
   2. insert data, time it
   3. some queries
3. add validation to nested schema
   1. flag dimension outliers
   2. flag duplicates geolocations using an index
4. Explore the dataset with semi-complex queries
   1. explain and add index to optimize
5. import the gardens dataset into its own collection
   1. Explore with some queries
6. Create a pipeline aggregation to Identify trees in a given area
7.  build a new collection, and compare the 3 schema design scenarios
   1. gardens : nested trees
   2. gardens : reference trees
   3. gardens : nested and outlier pattern


### dataset samples

- https://opendata.paris.fr/explore/dataset/les-arbres/information/
- https://opendata.paris.fr/explore/dataset/espaces_verts/information/


Here's a random sample of the trees dataset

```json
{
        "idbase":249403,
        "location_type":"Arbre",
        "domain":"Alignement",
        "arrondissement":"PARIS 20E ARRDT",
        "suppl_address":"54",
        "number":null,
        "address":"AVENUE GAMBETTA",
        "id_location":"1402008",
        "name":"Tilleul",
        "genre":"Tilia",
        "species":"tomentosa",
        "variety":null,
        "circumference":85,
        "height":10,
        "stage":"Adulte",
        "remarkable":"NON",
        "geo_point_2d":"48.86685102642415, 2.400262189227641"
    },
```

and a sample of the garden dataset

```json
    {
        "nom_de_espace_vert": "JARDINS DES CHAMPS ELYSEES - CARRE  DU THEATRE DU ROND-POINT",
        "typologie_espace_vert": "Promenades ouvertes",
        "categorie": "Jardin",
        "adresse_numero": 5.0,
        "adresse_complement": "X",
        "adresse_type_voie": "AVENUE DES",
        "adresse_libelle_voie": "CHAMPS ELYSEES",
        "code_postal": 75008.0,
        "presence_cloture": "Non",
        "annee_de_ouverture": 1928.0,
        "annee_de_changement_de_nom": 2024.0,
        "nombre_entites": 1.0,
        "ouverture_24h_24h": "Oui",
        "id_division": 104.0,
        "id_atelier_horticole": 9.0,
        "ida3d_enb": "46225",
        "site_villes": "3752",
        "id_eqpt": "4979",
        "competence": "CP",
        "geo_shape": "{\"coordinates\": [[[[2.3121917695809056, 48.868066632757014], [2.311966690887814, 48.86775855244269],
        ...,  [2.3108033322716635, 48.86794252502758],
        ]]], \"type\": \"MultiPolygon\"}",
        "url_plan": "https://b22-pr-v1-iis01.ressources.paris.mdp/MAAP.asp?object=46225",
        "geo_point": "48.86783322261344, 2.3111820973921144"
    },
```

### Expectations and work

Each worksheet step follows

- context
- goal statement
- example of script
- create your version
- assess the results
- submit the answer


We work by default in the terminal with mongosh.

But if you prefer to work with Mongo Compass or a driver (python, node.js, etc ...) please do so.
This work not about scripting. Feel free to use the best tool available for you.

## Step 1: trees and flat schema

Let's start simple with a flat schema. Each tree document only has one level of information.

The dataset is available [here]()

### Create a new database


Let's create a new database and call it `trees_flat_db`.

```bash
use trees_flat_db
```

and create a trees collection with

```bash
db.createCollection("trees")
```

check that the database has been created with

```bash
show dbs
```


Note: if you need space on your ATLAS free tier server, you cna drop a database (for instance `sample_airbnb`) with

```bash
# connect to the database
use sample_airbnb
# drop the db
db.dropDatabase('sample_airbnb')
```

Somehow you cannot drop a database that is not the current one.

## Let's load the data


We have sevarl options . including using the `mongoimport` from the command line. `mongoimport` needs to be installed separately. It is usually the fastest option for large volumes.

Here we use the `load()` function


Let's time the operation

```javascript
const fs = require("fs");

// Load and parse the JSON file
const dataPath = "./trees_flat.json"
const treesData = JSON.parse(fs.readFileSync(dataPath, "utf8"));

// Insert data into the desired collection
let startTime = new Date()
db.trees.insertMany(treesData);
let endTime = new Date()
print(`Operation took ${endTime - startTime} milliseconds`)
```

Write down that time somewhere.
On my mac it took  milliseconds for the entire 211k+ trees.

To delete all the documents from the trees collection

```javascript
db.treesdb.deleteMany({})
```

To check that all the rows have been inserted correctly

```javascript
db.trees.stats()
```

### Using mongoimport

using mongosh scripting may take too long

You can use mongoimport

make sure you load the ndjson formatted file (one document per line ) and not the json file (an array of documents)


mongoimport --uri="mongodb+srv://<username>:<password>@<cluster-url>" \
  --db <database_name> --collection trees --file ./trees_flat.ndjson --numInsertionWorkers 8 --batchSize 1000


time mongoimport --uri="mongodb+srv://<username>:<password>@<cluster-url>" \
  --db <database_name> --collection trees --file ./trees_flat.ndjson --numInsertionWorkers 8 --batchSize 1000

play with batchSize


### Queries

Explore the dataset, write the following queries

- Count trees per domain

```javascript
db.trees.aggregate([
  { $group: {
      _id: "$domain",
      count: { $sum: 1 }
  }},
  { $sort: { count: -1 }}
])
```

- Stats for remarkable trees

```javascript
db.trees.aggregate([
  { $match: {
      remarkable: "OUI",
      height: { $gt: 0 },
      circumference: { $gt: 0 }
  }},
  { $group: {
      _id: null,
      avgHeight: { $avg: "$height" },
      avgCircumference: { $avg: "$circumference" },
      count: { $sum: 1 }
  }}
])
```

- stats for non remarkable trees : field is missing


```javascript
db.trees.aggregate([
 { $match: {
     remarkable: { $exists: false },
     height: { $gt: 0 },
     circumference: { $gt: 0 }
 }},
 { $group: {
     _id: null,
     avgHeight: { $avg: "$height" },
     avgCircumference: { $avg: "$circumference" },
     count: { $sum: 1 }
 }}
])
```

- top 5 names of trees

```javascript
db.trees.aggregate([
 { $group: {
   _id: "$name",
   count: { $sum: 1 }
 }},
 { $sort: { count: -1 }},
 { $limit: 5 }
])
```


now let's focus on the Platane which is the most common tree in Paris,

What makes these remarkable tree

check the avg heaight and circ for Pllatanses


- Stats for Platane trees by remarkable status
- check also for Marronier

```javascript
db.trees.aggregate([
 { $match: { name: "Platane" }},
 { $group: {
   _id: { remarkable: { $ifNull: ["$remarkable", "NOT_SET"] }},
   count: { $sum: 1 },
   avgHeight: { $avg: "$height" },
   maxHeight: { $max: "$height" },
   avgCircumference: { $avg: "$circumference" }
 }},
 { $sort: { "_id.remarkable": 1 }}
])
```

So can you conclude what makes these trees remarkable ?

just for the sake of it ... add a field that calculates the age of the tree

the rule is very roughly

age in years = Circumference / π   / 1cm


```javascript
db.trees.aggregate([
 { $addFields: {
   age: { $divide: ["$circumference", 3.14159] }
 }}
])
```

and see the stats

```javascript
db.trees.aggregate([
  { $addFields: {
    age: { $divide: ["$circumference", 3.14159] }
  }},
  { $match: {
    circumference: { $gt: 0 }
  }},
  { $group: {
    _id: null,
    avgAge: { $avg: "$age" },
    minAge: { $min: "$age" },
    maxAge: { $max: "$age" }
  }}
])
```

you can use $set ot create the field age and make it permamnent

```javascript
db.trees.updateMany(
 { circumference: { $gt: 0 }},
 [{
   $set: {
     age: { $divide: ["$circumference", 3.14159] }
   }
 }]
)
```

Looking at the max age ... is there a problem ?


## different schema -

having everything in the same document is simple

but we can also use a different schema where we nest location, dimensions and taxonomy

```json
{
    "idbase": 249403,
    "domain": "Alignement",
    "stage": "Adulte",
    "location": {
        "arrondissement": "PARIS 20E ARRDT",
        "suppl_address": "54",
        "address": "AVENUE GAMBETTA",
        "id_location": "1402008",
        "geo_point_2d": "48.86685102642415, 2.400262189227641"
    },
    "dimensions": {
        "height": 10,
        "circumference": 85
    },
    "taxonomy": {
        "name": "Tilleul",
        "species": "tomentosa",
        "genre": "Tilia"
    }
}
```

although that complexifies a bit the queries, the data nature is clearer

as we've seen some trees have anomalies in their measurements

[TODO] start here

Write a schema validator that enforces the age to be less than 500 years
and height less than 100m

```javascript
db.createCollection("trees", {
 validator: {
   $jsonSchema: {
     bsonType: "object",
     required: ["height", "age"],
     properties: {
       height: {
         bsonType: "number",
         minimum: 0,
         maximum: 100
       },
       age: {
         bsonType: "number",
         minimum: 0,
         maximum: 500
       }
     }
   }
 }
})
```


Key points on writing these:

Use $group with $sum:1 for counting
Filter nulls/zeros with $match for accurate averages
Sort counts with $sort for better readability
Group by null when calculating global averages

##

write a schema and validator for that data

validator should enforce

- height >= 0 and <100
- stage ['']

## practice

take a json version of the trees dataaset where cols with null values have been remved

1. freedom rules
   - insert all the data without validation
   - query to check out the absurd values : height, geolocation
2. more controlled approach
   - write schema and validator: using MongoDB's $jsonSchema validator to "dry run" your data validation.
   - check number of skip documents
   - write a validator that gets as many documents as possible while keeping out absurd values
3. indexes
   - add a unique index on geolocation

in MongoDB, the schema validator is typically created along with the collection, not as a separate step

## conversations

### convert gardens geolocation

https://claude.ai/chat/eee56da3-bb07-4437-b71f-419059dbf80d

### transform point to GeoJSON

https://chatgpt.com/c/6753fcbe-20c8-800e-a730-a4be1208201c

```bash
db.collection.aggregate([
  {
    $addFields: {
      // Split the string into an array of [latitude, longitude]
      coordinates: {
        $map: {
          input: { $split: ["$location_string", ", "] },
          as: "coord",
          in: { $toDouble: "$$coord" } // Convert to numbers
        }
      }
    }
  },
  {
    $addFields: {
      // Create a GeoJSON object with reordered coordinates [longitude, latitude]
      location_geojson: {
        type: "Point",
        coordinates: [
          { $arrayElemAt: ["$coordinates", 1] }, // longitude
          { $arrayElemAt: ["$coordinates", 0] }  // latitude
        ]
      }
    }
  },
  {
    $unset: "coordinates" // Optionally remove the intermediate array field
  }
]);

```


### create collection, schema validator

also test validator, unicity check with index or within validator

see https://claude.ai/chat/a4154579-4fed-493f-8174-883ca8b3012d
