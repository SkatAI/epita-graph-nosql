- [S03.02  Paris Trees worksheet on MongoDB](#s0302--paris-trees-worksheet-on-mongodb)
  - [The context](#the-context)
  - [Work Plan](#work-plan)
    - [A look at the Datasets](#a-look-at-the-datasets)
      - [Download the data](#download-the-data)
      - [Tree sample](#tree-sample)
      - [Garden sample](#garden-sample)
    - [Expectations and work](#expectations-and-work)
      - [Local](#local)
      - [ATLAS cluster](#atlas-cluster)
      - [Connect](#connect)
      - [deliverables](#deliverables)
  - [Step 1: load the data](#step-1-load-the-data)
    - [Create a new database](#create-a-new-database)
    - [Let's load the data](#lets-load-the-data)
      - [In mongosh with `insertMany()`](#in-mongosh-with-insertmany)
      - [with `mongoimport`](#with-mongoimport)
    - [Load the entire trees dataset](#load-the-entire-trees-dataset)
    - [Data Exploration](#data-exploration)
      - [side note on checking trees](#side-note-on-checking-trees)
    - [Add Gardens](#add-gardens)
  - [Step 2 : validation and loading data](#step-2--validation-and-loading-data)
    - [Schema Validation](#schema-validation)
    - [Write the validator](#write-the-validator)
    - [duplicates](#duplicates)
    - [Unique index](#unique-index)
  - [take a Break : load the whole dataset](#take-a-break--load-the-whole-dataset)
    - [Cloning the collection](#cloning-the-collection)
  - [Step 3: GeoJSON](#step-3-geojson)
  - [conversions](#conversions)
    - [Convert the  geo\_point\_2d string into a Point](#convert-the--geo_point_2d-string-into-a-point)
    - [1st step : split the string into an array of floats](#1st-step--split-the-string-into-an-array-of-floats)
    - [2nd step convert the array of floats into a Point](#2nd-step-convert-the-array-of-floats-into-a-point)
    - [covert gardens geolocation](#covert-gardens-geolocation)
    - [add a geo spatial index](#add-a-geo-spatial-index)
    - [query](#query)
  - [Gardens](#gardens)

# S03.02  Paris Trees worksheet on MongoDB

## The context

Trees are a huge asset in the path to resilience with regard to climate change.

The city of Paris has a strong policy to leverage trees in its strategy to lower the temperatures during heat waves.


- "_The city has set itself the goal of de-bituminizing 100 hectares and planting 170,000 trees between 2020 and 2026._" from [Climate change resilience (fr)](https://www.paris.fr/pages/paris-s-adapte-au-changement-climatique-18541)
- see also "The tree plan" document <https://cdn.paris.fr/paris/2021/12/13/daf6cce214190a66c7919b34989cf1ed.pdf>

<div style='display: block; margin: auto; width : 50%;  padding: 10px;'>

<img src="./../img/trees-temperature.png"  style='display: block; margin: auto;' alt= "Trees lower the temperature"> from <a href="https://www.watson.ch/fr/suisse/meteo/257901657-les-arbres-une-solution-pas-si-simple-face-a-la-canicule-en-ville">this article</a>
</div>

You want to launch a new app where people can find shade in the event of a heat wave. Given a user location, the app finds the closest garden with the largest amount of trees.

You have an important meeting next week with the Paris team in charge of trees where you want to demonstrate a POC (proof of concept) of your new app.

Fortunately, there is a couple of datasets available on the Paris open data platform that you can leverage

- The [Paris Tree dataset](https://opendata.paris.fr/explore/dataset/les-arbres/information/) that includes over 210k trees in and around Paris.
- The [Espaces verts](https://opendata.paris.fr/explore/dataset/espaces_verts/information/) dataset (green spaces, aka gardens) with over 2300 areas.

The first phase of the POC is to use these 2 datasets to figure out how many trees each "Espace vert", aka green space, contains.

These are real world datasets and as such have their share of anomalies.

In today's workshop you will

[todo] update

- load both datasets into MongoDB collections and compare schema designs
- clean up the data by creating validation rules
- learn to use [GeoJSON](https://en.wikipedia.org/wiki/GeoJSON)
- find the best schema pattern to reconcile both dataset in one.

## Work Plan

The plan of this workshop

1. import the Paris trees dataset into a new MongoDB collection
   - explore it
   - remove outliers and duplicates
2. write a schema validator that excludes anomalies
   - test multiple ways to import the data

3. nested schema scenario for the trees dataset
   1. create schema
   2. insert data, time it
   3. some queries
4. add validation to nested schema
   1. flag dimension outliers
   2. flag duplicates geolocations using an index
5. Explore the dataset with semi-complex queries
   1. explain and add index to optimize
6. import the gardens dataset into its own collection
   1. Explore with some queries
7. Create a pipeline aggregation to Identify trees in a given area
8. build a new collection, and compare the 3 schema design scenarios
   1. gardens : nested trees
   2. gardens : reference trees
   3. gardens : nested and outlier pattern


### A look at the Datasets

#### Download the data

You can find the datasets in the github repo

Each dataset exists the following versions

- 100 samples: data/gardens_100.json.zip
- 1000 samples: data/gardens_1k.json.zip
- full dataset in JSON: data/gardens.json.zip
- full dataset in NDJSON: data/gardens.ndjson.zip

A `json` file contains an array of documents, while a `ndjson` file contains one document per line.

#### Tree sample

Before we begin here's a random sample of the trees dataset

```json
{
    "idbase": 121837,
    "domain": "Jardin",
    "stage": "Adulte",
    "height": 12,
    "circumference": 290,
    "location": {
        "id_location": "103006",
        "address": "JARDIN DE L HOSPICE DEBROUSSE / 148 RUE DE BAGNOLET",
        "arrondissement": "PARIS 20E ARRDT"
    },
    "geo": {
        "geo_point_2d": "48.86213187971267, 2.406612673774295"
    },
    "taxonomy": {
        "name": "If",
        "species": "baccata",
        "genre": "Taxus"
    }
}
```

Not all the fields are present in that sample. Some trees also have a `taxonomy.variety` field.

Note that the geo location value `geo_point_2d` is a string. We will have to convert it into a Point data type using an aggregation pipeline


#### Garden sample

Here's  a sample of the garden dataset

```json
{
    "nom": "JARDINIERES EVQ DE LA RUE WALDECK-ROUSSEAU",
    "typologie": "DÃ©corations sur la voie publique",
    "categorie": "Jardiniere",
    "nombre_entites": 4.0,
    "id": {
        "id_division": 117.0,
        "id_atelier_horticole": 34.0,
        "ida3d_enb": "JDE13017",
        "site_villes": "SV",
        "id_eqpt": "13017"
    },
    "location": {
        "adresse_numero": 1.0,
        "adresse_type_voie": "RUE",
        "adresse_libelle_voie": "WALDECK ROUSSEAU",
        "code_postal": 75017
    },
    "surface": {
        "superficie_totale_reelle": 45.0,
        "surface_horticole": 45.0
    },
    "history": {
        "annee_de_ouverture": 2024.0
    },
    "geo": {
        "geo_shape": {
            "coordinates": [
                [
                    [
                        [
                            2.286545191756581,
                            48.88003700060801
                        ],
                        // ...
                    ]
                ],
                [
                    [
                      // ...
                    ]
                ]
            ],
            "type": "MultiPolygon"
        },
        "geo_point": "48.87986720205693, 2.2864491642985567"
    }
}
```

### Expectations and work

We work by default in the terminal with `mongosh`.

If you prefer to work with Mongo Compass or a driver (python, node.js, etc ...) please do so.

This work not about scripting. Feel free to use the best tool available for you.

#### Local

You can either work on your local after installing mongo and mongosh

[TODO] provide docker file for local install

#### ATLAS cluster

or connect from your local to a ATLAS free cluster.

Atlas free tier clusters offer 512Mb for storage. However, if the current ATLAS free-tier cluster does not have enough space, you can delete it and create a new one that has no data.

#### Connect

Assuming you've set your MongoDB password as an environment variable `MONGODB_PASSWORD`, you can connect with

```bash
mongosh "mongodb+srv://<username>:${MONGODB_PASSWORD}@<server_uri>/"
```

or simply set the whole URI as an environment variable and connect with

```bash
mongosh ${MONGO_ATLAS_URI}
```

This connects you to the default `test` database.

#### deliverables

[TODO] are there any ?

## Step 1: load the data


### Create a new database

We  start by creating a new database, let's call it `treesdb`. In MongoDB, to create a new database you simply `use` it

```bash
use treesdb   # this creates the database
```

Here are a few examples of usefull MongoDB instructions.

- list databases on the cluster

You can check that the database has been created with `show dbs`

```bash
Atlas atlas-2914eu-shard-0 [primary] test> show dbs
admin  348.00 KiB
local    8.47 GiB
```

- Create a `trees` collection with

```bash
db.createCollection("trees")
```

- drop a database

To drop a database, first use it `use sample_mflix`  and  then you can drop it `db.dropDatabase('sample_mflix')`. For some reasons you cannot drop a database that is not the current one.


- Empty a  collection with `deleteMany()`

```js
db.trees.deleteMany({})
```

- delete a collection with `drop()`

```js
db.trees.drop()
```

- check the content of a collection with `stats()`

```js
db.trees.stats()
```

Here is a good [MongoDB cheatsheet](https://www.mongodb.com/developer/products/mongodb/cheat-sheet/) for other instructions.

### Let's load the data

We have several options to load the data from a JSON file. Either in mongosh with `insertMany()` or from the command line with `mongoimport`.

Since loading the whole dataset may take awhile, it's best to work on a 1k or 100 sample subset. All examples below use the 1k trees subset.

#### In mongosh with `insertMany()`

The following script

- loads the data from the `trees.json` file
- Insert data into the `trees` collection with `insertMany()`
- times the operation

```js
// load the JSON data
const fs = require("fs");
const dataPath = "./trees_1k.json"
const treesData = JSON.parse(fs.readFileSync(dataPath, "utf8"));

// Insert data into the desired collection
let startTime = new Date()
db.trees.insertMany(treesData);
let endTime = new Date()
print(`Operation took ${endTime - startTime} milliseconds`)
```

#### with `mongoimport`

`mongoimport`  is usually the fastest option for large volumes.

To check if `mongoimport` is already installed on your local, do `mongoimport --version`. If it returns the version and some info you're fine, otherwise you must install it.

See this page to download [command line utilities](https://www.mongodb.com/try/download/database-tools). The list of utilities is [here](https://www.mongodb.com/docs/database-tools/). It includes `mongoimport`, `mongoexport`. see also `mongostat` and `mongotop` as diagnostic tools.


`mongoimport` takes a ndjson file (one document per line ) as input by default. But you can also use a JSON file (an array of documents) and add the flag `--jsonArray`.

```bash
mongoimport --uri="mongodb+srv://<username>:<password>@<cluster-url>" \
--db <database_name> \
--collection <collection_name> \
--file <path to ndjson file>
```

In our context, here is a possible command line, using the `MONGO_ATLAS_URI` environment variable and loading the JSON file `trees_1k.json`.

```bash
time mongoimport --uri="${MONGO_ATLAS_URI}" \
--db treesdb  \
--collection trees \
--jsonArray \
--file ./trees_1k.json
```

Other interesting, self explanatory, flags that may come in handy:

- `--mode=[insert|upsert|merge|delete]`
- `--stopOnError`
- `--drop` (drops the collection first )
- `--stopOnError`

The `mongoimport` command line above results in

```bash
2024-12-13T11:41:45.941+0100	connected to: mongodb+srv://[**REDACTED**]@skatai.w932a.mongodb.net/
2024-12-13T11:41:48.942+0100	[########################] treesdb.trees	558KB/558KB (100.0%)
2024-12-13T11:41:52.087+0100	[########################] treesdb.trees	558KB/558KB (100.0%)
2024-12-13T11:41:52.087+0100	1000 document(s) imported successfully. 0 document(s) failed to import.
mongoimport --uri="${MONGO_ATLAS_URI}" --db treesdb --collection trees  --fil  0.15s user 0.09s system 3% cpu 6.869 total
```

### Load the entire trees dataset

If you are able to load the 1k sample file, you should now load the entire dataset. This takes a few minutes.

Make sure you remove all existing documents from the current `treesdb` database with `db.trees.deleteMany({})`.

Then import the whole dataset from the `trees.json` or the `trees.ndjson` file. (unzip first of course)

### Data Exploration

Explore the dataset, write the following queries

- Count trees per domain, order by most common domains first

This is equivalent of the SQL

```sql
select count(*) as n, domain from trees group by trees order n desc
```

[solution]

```js
db.trees.aggregate(
  {$group : {
    _id: "$domain",
    count : {$sum : 1}
  }},
  { $sort: {count : -1} }
)
```

```js
db.trees.aggregate(
  {
    $match : {
      height : {$gt: 50}
    }
  },
  {
    $sort : {height: 1}
  },
  {
    $limit : 1
  }
)
```

You should obtain 48256 Alignments for the most common domain and 33 for _DAC_ (not clear what DAC refers to).

- Count the number of trees per stage

The distinct function `db.trees.distinct('stage')` returns 4 values: `[ 'Adulte', 'Jeune (arbre)', 'Jeune (arbre)Adulte', 'Mature']`. However, the `Stage` field is often missing.

Count the number of trees per stage for all values as well as number of trees with missing stage (null values), order by descending.

[solution]

```js
db.trees.aggregate([
  {
    $group: {
      _id: "$stage",
      count: { $sum: 1 }
    }
  },
  {
    $sort: {count : -1}
  }
])
```

Note: using `$ifNull`, you can specify a string value for null.

```js
{
  $ifNull: ["$stage", "MISSING"] // Use "MISSING" as a placeholder for missing 'stage'
},
```

- dimensions of trees

Let's calculate the dimensions of the trees in the database. per stage.

Group the trees by stage and calculate the number, min, max and average height and circumference of trees.

- Exclude trees where the stage is missing.
- use `$project` and the [`$round` operator](https://www.mongodb.com/docs/manual/reference/operator/aggregation/round/) to round up the average to 2 decimals
- sort by average height descending

[solution]

```js
db.trees.aggregate(
  {
    $match : { stage: {$exists: true} }
  },
  {
    $group : {
      _id: "$stage",
      count: {$sum : 1},
      avgHeight: { $avg: "$height" },
      avgCircumference: { $avg: "$circumference" },
      minHeight: { $min: "$height" },
      minCircumference: { $min: "$circumference" },
      maxHeight: { $max: "$height" },
      maxCircumference: { $max: "$circumference" },
    }
  },
  {
    $project : {
      _id : 1,
      avgHeight: { $round: ["$avgHeight", 2] },
      minHeight : 1,
      maxHeight : 1,
      avgCircumference: { $round: ["$avgCircumference", 2] },
      minCircumference : 1,
      maxCircumference : 1,
    }
  },
  {
    $sort : {avgHeight: -1}
  }
)
```

- Stats for remarkable trees

Some trees are flagged with the field `remarkable`. If the field is present it has the value 'OUI', otherwise it's simply missing.

- count how many trees are remarkable

Write the query to count how many trees are remarkable. There are several ways to do that : using the fact that the field exists or filtering by its unique value 'OUI'.

[solution]

```js
db.trees.countDocuments( { remarkable : {$exists : true}  } )
// or
db.trees.countDocuments( { remarkable : 'OUI'  } )
```

- What makes a tree remarkable ?

We would assume that remarkable trees are special because of their dimensions. Let's try to verify that.

Write an aggregation pipeline that compares the average height and circumference of trees that are remarkable with ones that are not. What do you conclude, was our assumption correct?

In the aggregation pipeline:

- exclude trees that have zero height or zero circumference
- exclude trees that are higher than 100m or with a circumference > 500cm
- only consider mature trees (stage = 'Mature')
- only consider Platane trees (add `"taxonomy.name" : "Platane"` in the `$match` clause)

The output should show

- remarkable (OUI or null)
- count
- avgHeight
- avgCircumference

[solution]

```js
db.trees.aggregate(
  {
    $match : {
      stage : "Mature" ,
      "taxonomy.name" : "Platane",
      height : {$gt : 0},
      height : {$lt : 100},
      circumference : {$gt : 0},
      circumference : {$lt : 500}
    }
  },
  {
    $group : {
      _id : "$remarkable",
      count:  {$sum: 1},
      avgHeight : {$avg: "$height"},
      avgCircumference: { $avg: "$circumference" },
    }
  },
  {
    $project : {
      _id : 1,
      count : 1,
      avgHeight: { $round: ["$avgHeight", 2] },
      avgCircumference: { $round: ["$avgCircumference", 2] },
    }
  }
)
```

- top 5 names of trees

What are the most common trees in Paris ?

write the aggregation pipeline that returns the names of the top 5 most common trees names (`taxonomy.names`) in Paris

```javascript
db.trees.aggregate([
 { $group: {
   _id: "$taxonomy.name",
   count: { $sum: 1 }
 }},
 { $sort: { count: -1 }},
 { $limit: 5 }
])
```



- Calculate the age of the trees

The age of a tree is highly correlated to its circumference. The relation between age and circumference depends on many factors including the tree species. Here we use a very simplified rule where the age is the circumference divided by Ï.

> age in years = Circumference in cm / Ï

We want not only to calculate the age of each tree with but also add a new field `age` to the collection.

2 operators are available for that :

- $addFields: Only adds new fields, preserves existing ones
- $setFields (alias $set): Can both add new fields AND modify existing ones


But both operators only work in the current aggregation pipeline. They do not make persistent changes to the collection.

For that you need to use the `updateMany()` function.

see [this page](https://www.mongodb.com/docs/manual/tutorial/update-documents-with-aggregation-pipeline/) and [this one](https://www.mongodb.com/docs/manual/reference/method/db.collection.updateMany/#mongodb-method-db.collection.updateMany)  for the documentation and examples on using `updateMany()`

[solution]

```js
db.trees.updateMany(
  {},
  [
    {
      $set: {
        age: { $divide: ["$circumference", 3.14159] }
      }
    }
  ]
)
```

to check that the `age` field has been created, run `db.trees.countDocuments({age : {$exists: true}})`

#### side note on checking trees

As we've seen some trees have anomalies in their measurements.
Notably the max height is 2524 meters and the max circumference is 80105 cm. Most probably a human error.

Note: it's possible to visually check if a tree has the dimensions indicated in the dataset. Just search for the latitutde and longitude on google maps and check out the street photos. For instance this tree is supposed to have `circumference: 1650` but the photo shows only [normal sized Platanes}](https://www.google.com/maps/@48.8855265,2.3964855,3a,75y,84.69h,90t/data=!3m7!1e1!3m5!1s5q743M5eYOEsnIINCQBqfg!2e0!6shttps:%2F%2Fstreetviewpixels-pa.googleapis.com%2Fv1%2Fthumbnail%3Fcb_client%3Dmaps_sv.tactile%26w%3D900%26h%3D600%26pitch%3D0%26panoid%3D5q743M5eYOEsnIINCQBqfg%26yaw%3D84.68782!7i16384!8i8192?entry=ttu&g_ep=EgoyMDI0MTIwNC4wIKXMDSoASAFQAw%3D%3D).



### Add Gardens

Load now the gardens dataset into a gardens collection


```bash
time mongoimport --uri="${MONGO_ATLAS_URI}" \
--db treesdb  \
--collection gardens \
--jsonArray \
--file ./gardens.json
```

check that the import worked with `show collections` and `db.gardens.countDocuments()` which should return a total of 2313 documents.

- how many documents are gardens

Let's just check how many of these areas are gardens.

Write the aggregation pipeline to count the number of each `categorie`


[solution]

```js
db.gardens.aggregate(
  {
    $group : {
      _id : "$categorie",
      count : {$sum : 1}
    }
  },
  {
    $sort: { count : -1 }
  }
)
```

Among other categories, we have 19 Parcs, 2 Terrain de boules, 341 Murs vegetalises and 212 Jardins.

In what follows we call all these areas, simply gardens for simplicity sake.

For the sake of curiosity check the different `typologie`.

## Step 2 : validation and loading data

At this point you have a better understanding of the datasets and writing aggregation pipelines becomes more famliliar.

The next step is to make sure the data in the database is clean. And for that we will create the collection **before** importing the data. And we will specify what sort of data can be added to the collection.

### Schema Validation

First please familiarize yourself with schema validation in MongoDB with this document

https://github.com/SkatAI/epita-nosql-mongodb/blob/master/docs/schema_validation.md

see the documentation for a more in depth presentation https://www.mongodb.com/docs/manual/core/schema-validation/


We create the collection before importing the data with the function `db.createCollection(name, options)`.

The syntax for create collection can be found [here](https://www.mongodb.com/docs/manual/reference/method/db.createCollection/#syntax)

Here is a shorter version :

```js
db.createCollection( <name>,
    {
      validator: <document>,
    }
  )
```

> The validator option takes a `document` that specifies the validation rules or expressions.

Check out the example given [here](https://www.mongodb.com/docs/manual/core/schema-validation/specify-json-schema/#create-a-collection-with-validation)

### Write the validator

Your task is to write a validator that requires

- `height < 50`
- `circumference < 500`
- required `taxonomy.name` and `geo.geo_point_2d`
- `validationAction: "error"`

[solution]

```js
db.createCollection("trees", {
  validator: {
    $jsonSchema: {
      bsonType: "object",
      // required: ["height", "circumference"],
      required: [ "taxonomy.name", "geo.geo_point_2d", "height", "circumference"],
      properties: {
        height: {
          bsonType: "number",
          minimum: 0,
          maximum: 50,
          description: "Height must be between 0 and 50"
        },
        circumference: {
          bsonType: "number",
          minimum: 0,
          maximum: 500,
          description: "Circumference must be between 0 and 500"
        }
      }
    }
  },
  validationAction: "error"
})
```

before you import the data with `mongoimport` or with `db.trees.insertMany()`,

don't forget to empty the collection `db.trees.deleteMany({})`  or to drop it `db.trees.drop()` between each run.

your task: using `mongoimport` on the 1k sample dataset `trees_1k.json`

- import the dataset with validationAction: "error", and validationAction: "warn". what do you observe
- what happens if you don't drop or empty the collection between each run ?


[solution]

```bash
time mongoimport --uri="${MONGO_ATLAS_URI}" \
--db treesdb  \
--collection trees \
--jsonArray \
--file ./trees_1k.json
```

### duplicates

Besides crazy heights and insane circumferences, the dataset also holds geolocation duplicates.

- Write the aggregation pipeline that finds all the trees that share the same geolocation.

You should find 18 trees (9 pairs) that share the same geolocation if you are working with the the 1k or the whole dataset. (the dataset is designed so that duplicates show as the first documents )


[solution]

```js
db.trees.aggregate(
  {
    $group : {
      _id : "$geo.geo_point_2d",
      count : {$sum : 1}
    }
  },
  {
    $sort: { count : -1 }
  },
  {
    $limit : 10
  }
)

```


### Unique index

The best way to avoid duplicates is to add a unique index.

- drop the collection and recreate it with the validator
- add a unique index on the `geo.geo_point_2d` field

```js
db.trees.createIndex({ "geo.geo_point_2d": 1 }, { unique: true })
```

check that index exists with `db.trees.getIndexes()`. This should return

```js
[
  { v: 2, key: { _id: 1 }, name: '_id_' },
  {
    v: 2,
    key: { 'geo.geo_point_2d': 1 },
    name: 'geo.geo_point_2d_1',
    unique: true
  }
]
```

Now reimport the data

You should see messages like this one during the import.

```bash
2024-12-08T12:12:32.561+0100	continuing through error: E11000 duplicate key error collection: trees_flat_db.trees index: geo_point_2d_1 dup key: { geo_point_2d: "48.85002871626844, 2.3979943882939807" }
```

and after importing the data, if you rerun the aggregate pipeline for detecting  duplicates geolocations it should return nothing.



## take a Break : load the whole dataset

Take a break and before that load the whole dataset

but first:

- drop the trees collection
- create the collection with the validator
- create the index
- load the whole dataset using `trees.json` or `trees.ndjson`

[solution]

```bash
time mongoimport --uri="${MONGO_ATLAS_URI}" \
--db treesdb  \
--collection trees \
--jsonArray \
--file ./data-local/trees.json
```

or with the ndjson file.

```bash
time mongoimport --uri="${MONGO_ATLAS_URI}" \
--db treesdb  \
--collection trees \
--file ./data-local/trees.ndjson
```

This takes around 10 mn. Take a break

At the end you should see:

```bash
2024-12-08T12:44:27.147+0100 169120 document(s) imported successfully. 1231 document(s) failed to import.
```

Once that is done you should clone the collection so that if anything breaks you do not have to reload the whole dataset

### Cloning the collection

You can do so with

```javascript
db.trees.aggregate([
   { $match: {} },  // This matches all documents
   { $out: "trees_backup" }  // Creates a new collection named trees_backup
])
```

## Step 3: GeoJSON

next

- convert string to geojson
- find your address lat and long
- find number nearest trees given location
- do you have a remarkable tree ?


## conversions

The geolocation in the database is just a string with `<latitude>,<longitude>`.

Let's convert it to GeoJSON format.

see [geoJSON in MongoDB](https://www.mongodb.com/docs/manual/reference/geojson/) for reference

This will allow us to find the trees near a specific geolocation.

for instance find all the trees within 100 meters of the Eiffel tower with the `$near` operator ([documentation](https://www.mongodb.com/docs/manual/reference/operator/query/near/)).

```js
db.trees.find({
  location: {
    $near: {
      $geometry: {
        type: "Point",
        coordinates: [2.294694, 48.858093]  // Eiffel Tower coordinates
      },
      $maxDistance: 100  // in meters
    }
  }
})
```

or using the operator `$geoNear` ([documentation](https://www.mongodb.com/docs/manual/reference/operator/aggregation/geoNear/) :

```javascript
db.trees.aggregate([
  {
    $geoNear: {
      near: {
        type: "Point",
        coordinates: [2.294694, 48.858093]
      },
      distanceField: "distance",
      spherical: true,
      query: { "genre": "Tilia" }  // Only Linden trees
    }
  }
])
```

### Convert the  geo_point_2d string into a Point

But first we must convert the geo_point_2d string 'lat, long' into a geoJSON point.

Note: contrary to common speech where latitude comes before longitude, the geoJSON points have the longitude first and the latitude second

Build an aggregation pipeline that

- splits the `geo_point_2d` field into an array of 2 numbers into a temporary field `temp`
- then converts the array into a Point

### 1st step : split the string into an array of floats

We need to write a pipeline that will

- split the string on ',' into an array
- convert both value as numbers
-


> The `$addFields` stage adds new fields to documents without modifying the existing fields. In this case, we're creating a new field called "coordinates". Think of it like adding a new column to a spreadsheet, but one that's calculated from existing data.
Let's look at what's happening inside, working from the innermost operation outward:
> $split: ["$location_string", ", "]

> - This takes our input string like "48.86685102642415, 2.400262189227641"
> - The $split operator cuts this string wherever it finds ", " (comma followed by space)
> - The result is an array: ["48.86685102642415", "2.400262189227641"]

> The `$map` operator is like a transformation assembly line:
> - input: Takes the array we just created from splitting
> - as: "coord": For each piece of the array, temporarily names it "coord" (the $$ in $$coord is how we reference this temporary variable)
> - in: { $toDouble: "$$coord" }: This is the transformation applied to each piece


> - $toDouble converts each string number into an actual numeric value:


### 2nd step convert the array of floats into a Point

TODO:

do not update
create new field
and once the conversion has worked
delete old field

------------------------------
using `$set` and `$updateMany()` convert all geo_ooint_2dd into a Point data type

You will also

- need to swap latitude and longitude
- use the following operators
  - `$toDouble` to convert a string to a float
  - `$split` to split the geolocation string into an array with respect to the ','
  - `$arrayElemAt(array, index)` to get the ith element of the array

[solution]

```js
db.trees.updateMany(
  {},
  [{
      $set: {
        "geo": {
          "geo_point_2d": {
            type: "Point",
            coordinates: [
              { $toDouble: { $arrayElemAt: [{ $split: ["$geo_point_2d", ", "] }, 1] } },
              { $toDouble: { $arrayElemAt: [{ $split: ["$geo_point_2d", ", "] }, 0] } }
            ]
          }
        }
      }
    }]
)```

### rename

to be consistent with the geo point field in gardens let's rename geo_point_2d into a simple geo_point

using the updateMany() function and the `$rename` operator, rename `geo.geo_point_2d` as `geo.geo_point`

```js
db.trees.updateMany(
 {},
 { $rename: { "geo_point_2d": "geo.geo_point" } }
)
```


### covert gardens geolocation

do the same thing for the geolocation of the gardens

```js
db.gardens.updateMany(
  {},
  [
    {
      $set: {
        geo.geo_point: {
          type: "Point",
          coordinates: [
            { $toDouble: { $arrayElemAt: [{ $split: ["$geo_point_2d", ", "] }, 1] } },
            { $toDouble: { $arrayElemAt: [{ $split: ["$geo_point_2d", ", "] }, 0] } }
          ]
        }
      }
    }
  ]
)
```

### add a geo spatial index

before we can run geo queries we need to add a geo spatial index

```js
db.trees.createIndex({ geo.geo_point: "2dsphere" })
db.gardens.createIndex({ location_geojson: "2dsphere" })
```


### query


Now let's take an address

you can find the related longitude and latitude using google maps. just click right on the red pin

![lat long of Tricotin, best restaurant in the 13th arrondissement](./../img/tricotin-location.png)

you get for instance : 48.82124804336634, 2.363645912660704

Let's find the number of trees around this address, in a radius of 50m

```javascript
// Find how many trees are within 100 meters of Tricotin
db.trees.find({
  location_geojson: {
    $near: {
      $geometry: {
        type: "Point",
        coordinates: [2.363645912660704, 48.82124804336634]  // Tricotin coordinates
      },
      $maxDistance: 100  // in meters
    }
  }
}).count()
```

What about Epita ? with coordinates 48.815820795886815, 2.362806368481762

```javascript
// Find how many trees are within 100 meters of Epita
db.trees.find({
  location_geojson: {
    $near: {
      $geometry: {
        type: "Point",
        coordinates: [2.362806368481762, 48.815820795886815]  // Epita coordinates
      },
      $maxDistance: 100  // in meters
    }
  }
}).count()
```

only 11 trees!



What about your own address ?

Can you list the number of trees for each name ?

write the query that returns the number of trees by name around a 100m radius of uyour address

Just add a group by $name after teh first geo filter

```javascript
  {
   $group: {
     _id: "$name",
     count: { $sum: 1 }
   }
 },
```



```javascript
db.trees.aggregate([
 {
   $geoNear: {
     near: {
       type: "Point",
        // coordinates: [2.362806368481762, 48.815820795886815] // Epita
        // coordinates: [2.363645912660704, 48.82124804336634]  // Tricotin coordinates
        coordinates: [2.3368673391467576, 48.841417646345064]
    //    coordinates: [2.336245066633228, 48.84691090730081] // jardin du luxembourg
    //    coordinates: [2.348377228006662, 48.87940121485902]
     },
     distanceField: "distance",
     maxDistance: 100,
     spherical: true,
     key: "location_geojson"
   }
 },
 {
   $group: {
     _id: "$name",
     count: { $sum: 1 }
   }
 },
 {
   $sort: { count: -1 }
 }
]);
```

db.trees.aggregate([
 {
   $geoNear: {
     near: {
       type: "Point",
        coordinates: [2.3368673391467576, 48.841417646345064]
     },
     distanceField: "distance",
     maxDistance: 100,
     spherical: true,
     key: "location_geojson"
   }
 }
]);



db.trees.find({
  location_geojson: {
    $near: {
      $geometry: {
        type: "Point",
        coordinates: [2.3368673391467576, 48.841417646345064]
      },
      $maxDistance: 100  // in meters
    }
  }
}).count()



48.841417646345064, 2.3368673391467576


## Gardens

Next,
- load the gardens dataset into a gardens collection
- transform the geo_shape into a compatible geoJSON array of coordinates

- for a given garden find all the trees and their names
- what are the grdents with the highest number of trees ?
- what number fo trees could be considered as a threshold between normal gardens and ones with a large numbe rof trees

Next,

create a new collection of gardens and trees

for each garden add a nested collection of documents with all the trees in the garden

Does that exceed the 16mb limit for some gardens

Apply the outlier schema to gardens and trees using the threshold you have found before.
